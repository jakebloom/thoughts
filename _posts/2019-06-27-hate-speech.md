---
layout: post
title: How the tech companies painted themselves into a corner
description: Human problems require human solutions.
date: 27/06/2019
---

Facebook, as well as Twitter, YouTube and others are having trouble moderating their websites. How do I know this? Because those websites are on the internet and they are places for people to interact with each other. 

In March this year, these platforms were exposed badly, when a horrifically violent piece of content filmed in Christchurch was shared and re-uploaded millions of times. 

In response, these platforms have been very vocal about using Artificial Intelligence<sup>tm</sup> and Machine Learning<sup>tm</sup> to solve everything. Funnily enough, these responses are always light on implementation detail.

So let me tell you two things right now:

Firstly, there is no such thing as “Machine Learning” or “Artificial Intelligence”. Computers don’t learn and there is no way computers can be intelligent. Humans aren’t even intelligent. Whenever someone says “Machine Learning”, I want you to replace that phrase with “Pattern Recognition”. Because that’s all it is.

So we are planning to moderate humanity’s speech at scale using “Pattern Recognition”.

Second, a human problem will have a human solution. There is no app that is going to solve homelessness. No website that can really, truly, simulate human connection and intimacy. There is no social media campaign that will solve hate or stop people becoming radicalised.

These are all real, human problems that require culturally aware, civically minded, empathetic and, above all, human solutions.

The consequences of these limitations follow: using pattern recognition to police hate speech works well enough for existing patterns but will fail entirely for something new.

Pattern recognition is also vulnerable to things that look similar but are very different. Part of the virality of the Christchurch video would have been because it looked similar to a first person shooter video game. Facebook’s algorithm picked that up and said “this pattern looks like a video game, gaming content is usually popular, so let’s promote it!”. 

The “community guidelines” of these social media platforms are also a blunt instrument. Something is defined as “allowed” or “not allowed”, globally. There is very little room for nuance, or cultural differences across borders here. In practice, the rules are enforced in an opaque and inconsistent manner. As a higher proportion of content moderation is moved from human judegement to pattern recognition, it will only get more opaque and more inconsistent.

And finally, on the topic of hate speech - humans can’t put a consistent definition on hate speech, so there is no way that an algorithm will be able to.

So here we have the tech companies painted into a corner. They need things to put on recruiting materials, to say “we are working on some of the hardest problems with the coolest technology”. And they need to look busy fixing these problems to avoid getting hit hard by regulators. So Facebook, Twitter and YouTube will say that it is possible to use "Machine Learning" (pattern recognition) to police hate speech, when in reality this is an impossible task.

But what if it wasn’t impossible? What if we did actually develop an algorithm that determined, with one-hundred percent accuracy, what people are allowed to say and not allowed to say? Then what?

Then we have a computer defining how we can express ourselves. Is that the future we want to live in?